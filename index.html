<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="UTF-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0" />
		<title>WebSocket Audio Test</title>
		<style>
			body {
				font-family: sans-serif;
				margin: 20px;
			}
			#messages {
				width: 100%;
				height: 300px;
				border: 1px solid #ccc;
				margin-bottom: 10px;
				padding: 5px;
				box-sizing: border-box;
				overflow-y: scroll;
			}
			button {
				padding: 10px 15px;
				margin-right: 10px;
			}
			.status {
				margin-bottom: 10px;
			}
		</style>
	</head>
	<body>
		<h1>WebSocket Audio Test</h1>

		<div class="status">
			Connection Status: <span id="connectionStatus">Not Connected</span><br />
			Session ID: <span id="sessionId">N/A</span><br />
			Conversation ID: <span id="conversationId">N/A</span>
		</div>

		<button id="connectButton">Connect</button>
		<button id="disconnectButton" disabled>Disconnect</button>
		<hr />
		<button id="createConversationButton" disabled>Create Conversation</button>
		<button id="startRecordButton" disabled>Start Recording</button>
		<button id="stopRecordButton" disabled>Stop Recording</button>
		<button id="clearBufferButton">Clear Server Buffer (if needed)</button>
		<hr />
		<label for="vadMode">VAD Mode:</label>
		<select id="vadMode" disabled>
			<option value="enabled" selected>Enabled</option>
			<option value="disabled">Disabled</option>
		</select>
		<button id="updateSessionButton" disabled>Update Session</button>

		<audio id="streamingAudio" autoplay controls style="display: none;"></audio>

		<h2>Server Messages:</h2>
		<textarea id="messages" readonly></textarea>

		<script type="module">
			import audioDecode from "https://cdn.jsdelivr.net/npm/audio-decode@2.2.2/+esm";

			const connectButton = document.getElementById("connectButton");
			const disconnectButton = document.getElementById("disconnectButton");
			const createConversationButton = document.getElementById(
				"createConversationButton"
			);
			const startRecordButton = document.getElementById("startRecordButton");
			const stopRecordButton = document.getElementById("stopRecordButton");
			const clearBufferButton = document.getElementById("clearBufferButton");
			const vadModeSelect = document.getElementById("vadMode");
			const updateSessionButton = document.getElementById(
				"updateSessionButton"
			);
			let base64string = "";

			const messagesTextArea = document.getElementById("messages");
			const connectionStatusSpan = document.getElementById("connectionStatus");
			const sessionIdSpan = document.getElementById("sessionId");
			const conversationIdSpan = document.getElementById("conversationId");

			let socket = null;
			let currentSessionId = null;
			let currentConversationId = null;
			let audioContext = null; // Will be initialized on connect
			let ttsAudioContext = new AudioContext({ sampleRate: 48000 }); // For TTS audio playback

			// TTS playback scheduling
			const playbackQueue = [];
			let nextPlaybackTime = 0;

			// Variables for ScriptProcessorNode audio capture
			let scriptNode = null;
			let microphoneSourceNode = null;
			let userStream = null;

			// MediaSource for streaming audio
			let mediaSource = null;
			let sourceBuffer = null;
			let audioChunkQueue = [];
			let isStreamActive = false;
			let audioElement = null;
			let pendingAudioChunks = [];

			const WS_URL = "ws://localhost:3001"; // Your WebSocket server URL

			function logMessage(message, origin = "Client") {
				const timestamp = new Date().toLocaleTimeString();
				messagesTextArea.value += `[${timestamp} - ${origin}]: ${message}\n`;
				messagesTextArea.scrollTop = messagesTextArea.scrollHeight;
			}

			// Helper function to convert AudioBuffer to WAV
			function updateUIState(
				isConnected,
				conversationStarted = false,
				isRecording = false
			) {
				connectButton.disabled = isConnected;
				disconnectButton.disabled = !isConnected;
				createConversationButton.disabled = !isConnected || conversationStarted;
				vadModeSelect.disabled = !isConnected;
				updateSessionButton.disabled = !isConnected;

				startRecordButton.disabled =
					!isConnected || !conversationStarted || isRecording;
				stopRecordButton.disabled =
					!isConnected || !conversationStarted || !isRecording;
				clearBufferButton.disabled = !isConnected;
			}

			connectButton.addEventListener("click", () => {
				if (socket && socket.readyState === WebSocket.OPEN) {
					logMessage("Already connected.");
					return;
				}
				socket = new WebSocket(WS_URL);

				// Initialize MediaSource streaming
				setupStreamingAudio();

				socket.onopen = () => {
					connectionStatusSpan.textContent = "Connected";
					logMessage("Connected to WebSocket server.");
					if (!audioContext) {
						audioContext = new (window.AudioContext ||
							window.webkitAudioContext)();
						logMessage("AudioContext initialized.");
					}
					updateUIState(true);
				};

				socket.onmessage = (event) => {
					const messageData = JSON.parse(event.data);
					if (messageData.type === "response.audio.delta") {
						logMessage(
							`Audio response received with length: ${
								messageData.audio?.length || 0
							}`,
							"Client"
						);
						base64string = base64string + messageData.audio;
						// Play the incoming audio
						playTTSAudio(
							messageData.audio,
							messageData.mimeType || "audio/mpeg"
						);
						return;
					}
					logMessage(JSON.stringify(messageData, null, 2), "Server");

					if (messageData.type === "session.created") {
						currentSessionId = messageData.sessionId;
						sessionIdSpan.textContent = currentSessionId;
					}
					if (messageData.type === "conversation.created") {
						currentConversationId = messageData.conversationId;
						conversationIdSpan.textContent = currentConversationId;
						updateUIState(true, true);
					}
				};

				socket.onclose = (event) => {
					connectionStatusSpan.textContent = "Disconnected";
					logMessage(`Disconnected: ${event.reason || "No reason given"}`);
					currentSessionId = null;
					currentConversationId = null;
					sessionIdSpan.textContent = "N/A";
					conversationIdSpan.textContent = "N/A";
					updateUIState(false);
					
					// Clean up MediaSource resources
					cleanupMediaSource();
					
					// Ensure audio resources are cleaned up if connection drops mid-recording
					if (scriptNode) scriptNode.disconnect();
					if (microphoneSourceNode) microphoneSourceNode.disconnect();
					if (userStream)
						userStream.getTracks().forEach((track) => track.stop());
					scriptNode = null;
					microphoneSourceNode = null;
					userStream = null;
				};

				socket.onerror = (error) => {
					logMessage(
						`WebSocket Error: ${error.message || "Unknown error"}`,
						"Error"
					);
					console.error("WebSocket Error:", error);
					updateUIState(false);
				};
			});

			disconnectButton.addEventListener("click", () => {
				if (socket) {
					socket.close(); // This will trigger onclose where recording cleanup happens
				}
				
				// Clean up streaming resources
				cleanupMediaSource();
			});

			createConversationButton.addEventListener("click", () => {
				if (socket && socket.readyState === WebSocket.OPEN) {
					const msg = {
						type: "conversation.create",
						chatId: `chat-${Date.now()}`,
					};
					socket.send(JSON.stringify(msg));
					logMessage(JSON.stringify(msg));
				} else {
					logMessage("Not connected.");
				}
			});

			updateSessionButton.addEventListener("click", () => {
				if (socket && socket.readyState === WebSocket.OPEN) {
					const msg = {
						type: "session.update",
						vadMode: vadModeSelect.value,
					};
					socket.send(JSON.stringify(msg));
					logMessage(JSON.stringify(msg));
				} else {
					logMessage("Not connected.");
				}
			});

			startRecordButton.addEventListener("click", async () => {
				if (!currentConversationId) {
					logMessage("Please create a conversation first.");
					return;
				}
				if (navigator.mediaDevices && navigator.mediaDevices.getUserMedia) {
					try {
						// Create AudioContext with 16kHz sample rate specifically for STT
						audioContext = new AudioContext({ sampleRate: 16000 });
						logMessage("AudioContext initialized with 16kHz sample rate.");

						// Request high-quality audio with noise reduction
						userStream = await navigator.mediaDevices.getUserMedia({
							audio: {
								sampleRate: 16000,
								channelCount: 1,
								echoCancellation: true,
								noiseSuppression: true,
								autoGainControl: true,
							},
						});

						microphoneSourceNode =
							audioContext.createMediaStreamSource(userStream);
						// Use smaller buffer size for lower latency (1024 samples)
						const bufferSize = 1024;
						const inputChannels = 1;
						const outputChannels = 1;

						scriptNode = audioContext.createScriptProcessor(
							bufferSize,
							inputChannels,
							outputChannels
						);

						scriptNode.onaudioprocess = async (audioProcessingEvent) => {
							const audioData =
								audioProcessingEvent.inputBuffer.getChannelData(0);
							sendAudioChunk(audioData, audioContext.sampleRate);
						};

						microphoneSourceNode.connect(scriptNode);

						scriptNode.connect(audioContext.destination);

						logMessage(
							"Recording started using ScriptProcessorNode. Sending 16-bit WAV at 16kHz."
						);
						updateUIState(true, true, true);
					} catch (err) {
						logMessage(`Error setting up recording: ${err.message}`, "Error");
						console.error("Error in startRecordButton:", err);
						if (userStream) {
							userStream.getTracks().forEach((track) => track.stop());
							userStream = null;
						}
						// Attempt to restore a sensible UI state if start failed
						updateUIState(true, !!currentConversationId, false);
					}
				} else {
					logMessage("getUserMedia not supported on your browser!", "Error");
				}
			});

			async function sendAudioChunk(audioData, sampleRate) {
				if (socket && socket.readyState === WebSocket.OPEN) {
					const base64Chunk = base64EncodeAudio(audioData);
					const msg = {
						type: "input_audio_buffer.append",
						chunk: base64Chunk,
					};

					socket.send(JSON.stringify(msg));
				}
			}

			function floatTo16BitPCM(float32Array) {
				const buffer = new ArrayBuffer(float32Array.length * 2);
				const view = new DataView(buffer);
				let offset = 0;
				for (let i = 0; i < float32Array.length; i++, offset += 2) {
					let s = Math.max(-1, Math.min(1, float32Array[i]));
					view.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7fff, true);
				}
				return buffer;
			}

			function base64EncodeAudio(float32Array) {
				const arrayBuffer = floatTo16BitPCM(float32Array);
				let binary = "";
				let bytes = new Uint8Array(arrayBuffer);
				const chunkSize = 0x8000; // 32KB chunk size
				for (let i = 0; i < bytes.length; i += chunkSize) {
					let chunk = bytes.subarray(i, i + chunkSize);
					binary += String.fromCharCode.apply(null, chunk);
				}
				return btoa(binary);
			}

			stopRecordButton.addEventListener("click", () => {
				if (scriptNode) {
					scriptNode.disconnect();
					scriptNode.onaudioprocess = null;
					scriptNode = null;
				}
				if (microphoneSourceNode) {
					microphoneSourceNode.disconnect();
					microphoneSourceNode = null;
				}
				if (audioContext) {
					audioContext.close(); // Properly close the AudioContext
					audioContext = null;
				}
				if (userStream) {
					userStream.getTracks().forEach((track) => track.stop());
					userStream = null;
				}

				logMessage("Stopped recording and released audio resources.");
				updateUIState(true, true, false);

				if (socket && socket.readyState === WebSocket.OPEN) {
					const msg = { type: "input_audio_buffer.commit" };
					socket.send(JSON.stringify(msg));
					logMessage(JSON.stringify(msg));
				}
			});

			clearBufferButton.addEventListener("click", () => {
				if (socket && socket.readyState === WebSocket.OPEN) {
					const msg = { type: "input_audio_buffer.clear" };
					socket.send(JSON.stringify(msg));
					logMessage(JSON.stringify(msg));
				} else {
					logMessage("Not connected.");
				}
			});

			// TTS decode and scheduling
			async function decodeAndQueueTTS(audioBase64) {
				try {
					if (ttsAudioContext.state !== 'running') {
						await ttsAudioContext.resume();
						logMessage('TTS AudioContext resumed to schedule playback');
					}
					const byteCharacters = atob(audioBase64);
					const byteNumbers = new Array(byteCharacters.length);
					for (let i = 0; i < byteCharacters.length; i++) {
						byteNumbers[i] = byteCharacters.charCodeAt(i);
					}
					const byteArray = new Uint8Array(byteNumbers);
					const audioBuffer = await ttsAudioContext.decodeAudioData(byteArray.buffer);
					playbackQueue.push(audioBuffer);
					schedulePlayback();
				} catch (error) {
					console.error('Error decoding TTS audio:', error);
					logMessage(`Error decoding TTS audio: ${error.message}`, 'Error');
				}
			}

			function schedulePlayback() {
				if (playbackQueue.length === 0) return;
				const now = ttsAudioContext.currentTime;
				if (nextPlaybackTime < now) {
					nextPlaybackTime = now;
				}
				while (playbackQueue.length > 0) {
					const buffer = playbackQueue.shift();
					const source = ttsAudioContext.createBufferSource();
					source.buffer = buffer;
					source.connect(ttsAudioContext.destination);
					source.start(nextPlaybackTime);
					logMessage(`Scheduled TTS chunk at ${nextPlaybackTime.toFixed(2)}s`);
					nextPlaybackTime += buffer.duration;
				}
			}

			// Replace the old playTTSAudio function
			function playTTSAudio(audioBase64, mimeType = 'audio/mpeg') {
				if (!isStreamActive || !mediaSource || !sourceBuffer) {
					logMessage("Streaming not initialized, falling back to scheduling audio", "Warning");
					decodeAndQueueTTS(audioBase64);
					return;
				}
				
				try {
					// Convert base64 to ArrayBuffer
					const byteCharacters = atob(audioBase64);
					const byteNumbers = new Array(byteCharacters.length);
					for (let i = 0; i < byteCharacters.length; i++) {
						byteNumbers[i] = byteCharacters.charCodeAt(i);
					}
					const byteArray = new Uint8Array(byteNumbers);
					
					// Queue the chunk for processing
					pendingAudioChunks.push(byteArray.buffer);
					logMessage(`Added chunk to streaming queue (${pendingAudioChunks.length} chunks waiting)`);
					
					// If sourceBuffer is not busy, process immediately
					if (!sourceBuffer.updating) {
						processAudioChunk();
					}
				} catch (error) {
					logMessage(`Error processing audio for streaming: ${error.message}`, "Error");
					console.error("Streaming error:", error);
				}
			}

			// Initialize MediaSource for streaming audio
			function setupStreamingAudio() {
				audioElement = document.getElementById("streamingAudio");
				if (mediaSource) {
					// Clean up existing MediaSource if any
					if (sourceBuffer) {
						mediaSource.removeSourceBuffer(sourceBuffer);
						sourceBuffer = null;
					}
					pendingAudioChunks = [];
				}
				
				mediaSource = new MediaSource();
				audioElement.src = URL.createObjectURL(mediaSource);
				
				mediaSource.addEventListener('sourceopen', () => {
					logMessage("MediaSource opened and ready for streaming");
					try {
						// Use MP3 codec for compatible streaming
						sourceBuffer = mediaSource.addSourceBuffer('audio/mpeg');
						sourceBuffer.mode = 'sequence';
						
						sourceBuffer.addEventListener('updateend', () => {
							// Process any pending chunks when buffer update completes
							if (pendingAudioChunks.length > 0 && !sourceBuffer.updating) {
								processAudioChunk();
							}
						});
						
						isStreamActive = true;
						logMessage("Audio streaming initialized and ready");
					} catch (e) {
						logMessage(`MediaSource setup error: ${e.message}`, "Error");
						console.error("MediaSource error:", e);
					}
				});
			}

			// Process and append audio chunks to the source buffer
			function processAudioChunk() {
				if (!sourceBuffer || sourceBuffer.updating || pendingAudioChunks.length === 0) {
					return;
				}
				
				try {
					const chunk = pendingAudioChunks.shift();
					sourceBuffer.appendBuffer(chunk);
				} catch (e) {
					logMessage(`Error appending to buffer: ${e.message}`, "Error");
					console.error("Buffer append error:", e);
					// Try to handle buffer full error by resetting everything
					if (e.name === "QuotaExceededError") {
						logMessage("Buffer full, resetting source buffer", "Warning");
						// Remove old data to make room for new
						const currentTime = audioElement.currentTime;
						if (sourceBuffer.buffered.length > 0 && 
							currentTime > sourceBuffer.buffered.start(0) + 1) {
							sourceBuffer.remove(sourceBuffer.buffered.start(0), currentTime - 1);
						}
					}
				}
			}

			// Function to clean up MediaSource resources
			function cleanupMediaSource() {
				if (mediaSource && mediaSource.readyState === 'open') {
					try {
						if (sourceBuffer) {
							mediaSource.removeSourceBuffer(sourceBuffer);
						}
						mediaSource.endOfStream();
					} catch (e) {
						console.error("Error cleaning up MediaSource:", e);
					}
				}
				sourceBuffer = null;
				isStreamActive = false;
				pendingAudioChunks = [];
				
				if (audioElement) {
					audioElement.src = '';
				}
			}

			updateUIState(false);
		</script>
	</body>
</html>
